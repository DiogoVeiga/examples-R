# Accessing low level data with R/Bioconductor


# Working local

Let us suppose you would rather work with your local machine. Well, we can
still use docker and google cloud services to get some research done!

So, first, make sure you've got VirtualBox VM and docker installed, and both running.
If you're on a mac, that means starting the daemon (with the Docker Quickstart Terminal app).
That's going to take care of getting the VM running and configuring the terminal
environment.

Or use the terminal command
```
docker-machine create --driver virtualbox default
docker-machine env default
```

With that done, you should see something like:

```
Starting machine default...
(default) OUT | Starting VM...
...
                        ##         .
                  ## ## ##        ==
               ## ## ## ## ##    ===
           /"""""""""""""""""\___/ ===
      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ /  ===- ~~~
           \______ o           __/
             \    \         __/
              \____\_______/


docker is configured to use the default machine with IP xxx.yyy.zz.www
For help getting started, check out the docs at https://docs.docker.com
```

Then you can get the status:

```
docker-machine status default
```

And we can SSH into the docker machine:

```
docker-machine ssh default
```

Now, since the terminal env points to the VM, we can pull down the
Bioconductor docker image

```
docker pull bioconductor/release_microarray
```

That's going to download a pretty large amount of stuff (in the 3.x GBs), so
now might be time to refill your coffee.

When the download and extraction finishes, we can start up the docker container

```
docker run -ti --privileged bioconductor/release_microarray /bin/bash
```

The -i starts interactive dockers and -t starts a pseudo TTY so we can
interact with R/Bioconductor! You are now within a docker container.
Relish the moment. OK, stop relishing.

The first thing I'm going to do is install gcloud, which is really quick

```
curl https://sdk.cloud.google.com | bash

exec -l $SHELL
```

Then we can get authenticated by opening a browser and pasting the long
address, bringing back the auth string, and entering the project ID.

```
gcloud init
```

We can take a look what buckets are available to us:

```
gsutil ls
```


OK, now, we need to be able to access data that sitting in a restricted
bucket. To do that we're going to use gcsfuse to mount a google bucket
on our system.

https://github.com/GoogleCloudPlatform/gcsfuse

```
apt-get install fuse curl daemon
curl -L -O https://github.com/GoogleCloudPlatform/gcsfuse/releases/download/v0.12.0/gcsfuse_0.12.0_amd64.deb
sudo dpkg --install gcsfuse_0.12.0_amd64.deb
```

*** At this point we might want to save the state of our docker image so we
don't have to keep installing these package each time we use it. ***

First we'll try to mount some open access SNP arrays

```
mkdir /media/dat
daemon -- gcsfuse isb-cgc-open  /media/dat
cd /media/dat/ccle/SNP_Arrays
ls
```

OK! Let's fire up R, and see if we can read a file!

```
#R
library(affy)
affyData <- ReadAffy("AWASH_p_NCLE_DNAAffy10_GenomeWideSNP_6_E04_542932.CEL", verbose=T)
```

Success!!






# Working In the Cloud


In this work, we will run a docker image containing
R and a Bioconductor snapshot, and then access some raw data in a Google bucket.

Fortunately, starting up a cloud instance with everything you need
is pretty easy. Start out by pointing your browser to:

> http://googlegenomics.readthedocs.org/en/latest/use_cases/run_familiar_tools/bioconductor.html

On that page, there's a link to information about the docker containers
maintained by Bioconductor, and another link called "click-to-deploy Bioconductor".
That's the one we want. Click away!

That's going bring you to the Google Developer Console where you can select the
your current project. If you don't have a project ID yet, make sure to sign up,
and get that free compute time!

I'm going to select a 40GB disk and the "bioconductor/release_microarray" image. After you
click-to-deploy, you get a screen telling us that it's going to take 5-15
minutes to boot. It's a good time to make some coffee. Or if you haven't
installed Google gcloud yet, you can do that (https://cloud.google.com/sdk/).

Once the instance has loaded, we can SSH into it using gcloud compute ssh (https://cloud.google.com/sdk/gcloud/reference/compute/ssh)
or the web console interface (from the google developers console,
  click on compute engine and find the appropriate VM SSH link).
  We can also SSH *into* the docker image running R by using gcloud docker

  To log into RStudioServer, first use the following gcloud command on your local workstation to create an ssh tunnel between your local workstation and the Compute Engine instance:
  gcloud compute ssh --ssh-flag="-L8787:localhost:8787" --ssh-flag="-f" --ssh-flag="-N" --project=isb-cgc-02-0001 --zone us-central1-f bioconductor1-vm

  Then navigate to http://localhost:8787 and login with username:rstudio and password:rstudio.

  If you would like to open a bash shell in the docker image, first use the SSH button above to connect to the instance. Then from the Compute Engine instance, run the following command to invoke bash from within the Docker image:
  sudo docker run -ti --user rstudio --workdir /home/rstudio --volume /mnt/data:/home/rstudio/data bioconductor/release_microarray:latest bash
